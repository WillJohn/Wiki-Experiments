#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jan 3 10:08:28 2018

@author: xiangxiang
"""

"""
MLP keras test
"""

import numpy as np
import matplotlib.pyplot as plt

from keras.models import Sequential, Model
from keras.layers import Dense, Activation
from keras.optimizers import SGD

from sklearn.preprocessing import OneHotEncoder
plt.ion()

def GenerateSamples(xDim, yCard, nPerClass, nSamples):
    """
    Generate samples of data (X, Y).
    X is a vector with dimension xDim, Y is corresponding discrete label taking value from {0, 1, 2, ..., yCard - 1}
    The distribution of X|Y=i is a mixture Gaussian with nPerClass mixtures.
    Input args:
        xDim: dimension of X
        yCard: Cardinality of Y
        nPerClass: # of mixtures within each class
        nSamples: # of points generated

    return: [X, Y]
        X: n samples with dimension xDim
        Y: Corresponding Labels
    """
    u1 = np.random.choice(yCard, nSamples) # u1 indicates the label of class
    u2 = np.random.choice(nPerClass, nSamples) # u2 indicates the id of mixture in each class
    """ Centers of these mixtures are also generated by Gaussian distribution  """
    mu_c = 0 # mean of center
    sigma_c = 1 # deviation of center
    center = np.random.normal(mu_c, sigma_c, (yCard * nPerClass, xDim))
    mu_x = 0 # mean of each gaussin
    sigma_x = .2 # deviation of each gaussian
    X = np.random.normal(mu_x, sigma_x, (nSamples, xDim)) + center[u1 * nPerClass + u2, :]
    Y = u1    
    return([X, Y])
    
def MakeLabels(X):
    '''
    return onehotencoded array
    
    Input X should be an array (nSamples, 1), taking values from alphabet 
    with size xCard
    
    X must be labels themselves, i.e. integers starting from 0, with every 
    value used. Otherwise need to use sklearn.LabelEncoder()
    
    return array of size (nSamples, xCard)
    '''    
    onehot_encoder = OneHotEncoder(sparse=False)
    temp = X.reshape(len(X), 1)
    onehots = onehot_encoder.fit_transform(temp)
    return(onehots)

def h_score(x_mat, label, b_onehot = False):
    """
    Calculate The coffiencient of Separating
    x_mat: Each row of x_mat represents an observation, and each column a single variable in these samples
    y: label( could be label or one-hot encoded label; dafault: not one-hot)
    """
    label = np.array(label)
    if b_onehot:
        label = np.array([label_c.argmax() for label_c in label])
    label_set = np.unique(label)
    x_mean = np.mean(x_mat, axis = 0) # E[X]
    cov_x = np.cov(x_mat.T)           # Cov(X)
    pinv_cov_x = np.linalg.pinv(cov_x) # pinv(Cov(X))
    coe = 0
    """
    Here we use this formula:
    H-score = E[(E[X|Y] - E[X]) * inv(Cov(X)) * (E[X|Y] - E[X])]
    """
    for label_i in label_set: # current label i
        x_i = x_mat[label == label_i, :]      # get X where corresponding Y = i
        x_i_mean = np.mean(x_i, axis = 0)     # E[X|Y = i]
        x_i_mean_c = x_i_mean - x_mean        # E[X|Y = i] - E[X]
        pr_i = float(sum(label == label_i)) / len(label)  # Pr(Y = i)
        if cov_x.size > 1:
            coe_i = np.dot(x_i_mean_c, np.matmul(pinv_cov_x, x_i_mean_c))
            # (E[X|Y = i] - E[X]) * inv(Cov(X)) * (E[X|Y = i] - E[X])
        else:
            coe_i = x_i_mean_c ** 2 / cov_x
        coe = coe + coe_i * pr_i
    return coe

nSamples = 100000

nPerClass = 6
# If nPerClass is large (like 20, 30), the h-score and accuracy would all be low, but we can still see the mononically increasing trends of h-score over layers
yCard = 6
# too lazy to find more colors in plotting, so yCard needs to be no greater than 7. If yCard > 7, you need to expand color_list
dim_list = [6, 14, 12, 10, 8, 6] # Dimension list of NN, could be modified accordingly
xDim = dim_list[0] # Making xDim larger leads to easier separating -- Bless of dimensionality :) 
[X, Y] = GenerateSamples(xDim, yCard, nPerClass, nSamples)
plt.figure()
color_list = np.array(['r', 'g', 'b', 'c', 'm', 'k', 'y']) # colors for each class
plt.scatter(X[:, 0], X[:, 1], s = .1, c = color_list[[int(y) for y in Y]]) # plot the first 2 dimension of X with Y indicated by different colors.
plt.axis('equal')
plt.title('Data X (First two dimensions)')
plt.draw()

YLabels = MakeLabels(Y)

model = Sequential()

for id in range(len(dim_list) - 1):
    model.add(Dense(dim_list[id + 1], activation = 'relu', input_dim=dim_list[id]))
model.add(Dense(yCard, activation='softmax', input_dim=dim_list[-1]))

sgd = SGD(.2, decay=1e-6, momentum=0.9, nesterov=True, clipvalue=0.04)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])
nEpochs = 200
nSteps = 4
plt.figure() # Plot the trends of H-score over layers during the training process 
t_model = Model(inputs=model.input, outputs=[layer.input for layer in model.layers])
# t_model is used to get outputs from each layer.
s_list = t_model.predict(X) # list of outputs from all layers
h_score_list = [h_score(s, Y) for s in s_list]
acc = model.evaluate(X, YLabels, verbose = 0)[1]
plt.plot(range(len(h_score_list)), h_score_list, label='Epochs =   0' + ', Acc = ' + f"{acc:.3f}")
for i in range(nSteps):
    hist = model.fit(X, YLabels, verbose=0, batch_size=nSamples, epochs=int(nEpochs/nSteps))
    t_model = Model(inputs=model.input, outputs=[layer.input for layer in model.layers])
    # t_model is used to get outputs from each layer.
    s_list = t_model.predict(X) # list of outputs from all layers
    h_score_list = [h_score(s, Y) for s in s_list]
    acc = model.evaluate(X, YLabels, verbose = 0)[1]
    epochs = (i+1) * int(nEpochs/nSteps)
    plt.plot(range(len(h_score_list)), h_score_list, label='Epochs = ' + f"{epochs:3d}" + ', Acc = ' + f"{acc:.3f}")
plt.legend(loc='lower left')
plt.title('H-score over layers')
